{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "zR3Vib7hkLkC"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ufiaTOV4ka2b"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "rnn_layer = nn.RNN(input_size=5, hidden_size=2, num_layers=1, batch_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kwa23o9cwW7E"
   },
   "source": [
    "Definimos un módulo recurrente, con una única capa con 2 neuronas, que se conecta con 5 neuronas (features) de entrada.\n",
    "Veamos como quedan sus parámetros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YgEBh1vswolq",
    "outputId": "0f679de4-79cd-427e-b148-5e8584408497"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight_ih_l0: Parameter containing:\n",
      "tensor([[ 0.3643, -0.3121, -0.1371,  0.3319, -0.6657],\n",
      "        [ 0.4241, -0.1455,  0.3597,  0.0983, -0.0866]], requires_grad=True)\n",
      "weight_hh_l0: Parameter containing:\n",
      "tensor([[ 0.1961,  0.0349],\n",
      "        [ 0.2583, -0.2756]], requires_grad=True)\n",
      "bias_ih_l0: Parameter containing:\n",
      "tensor([-0.0516, -0.0637], requires_grad=True)\n",
      "bias_hh_l0: Parameter containing:\n",
      "tensor([ 0.1025, -0.0028], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for param, value in rnn_layer.named_parameters():\n",
    "    print(f\"{param}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NhJjprp8kSNJ",
    "outputId": "13e22fe9-66c4-4bb0-a101-0a69b683ee61"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W_xh shape: torch.Size([2, 5])\n",
      "W_hh shape: torch.Size([2, 2])\n",
      "b_xh shape: torch.Size([2])\n",
      "b_hh shape: torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "w_xh = rnn_layer.weight_ih_l0\n",
    "w_hh = rnn_layer.weight_hh_l0\n",
    "b_xh = rnn_layer.bias_ih_l0\n",
    "b_hh = rnn_layer.bias_hh_l0\n",
    "print('W_xh shape:', w_xh.shape)\n",
    "print('W_hh shape:', w_hh.shape)\n",
    "print('b_xh shape:', b_xh.shape)\n",
    "print('b_hh shape:', b_hh.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2p3-s7EwW7F"
   },
   "source": [
    "Vemos que tenemos:\n",
    "- una matriz con los pesos que unen las 2 neuronas con los 5 inputs.\n",
    "- una matriz con los pesos de los estados anteriores de las 2 neuronas con ellas mismas en el paso actual.\n",
    "- los 2 sesgos de las conexiones de entrada (uno para cada neurona)\n",
    "- los 2 sesgos de las conexiones recurrentes (uno para cada neurona)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ifAgnN_mBMz"
   },
   "source": [
    "Creamos una (1) única instancia con una secuencia 3 pasos de tiempo, con los datos de 5 features (e.g. temperatura, presión, precipitación, CO2, material particulado). Para simplificar las 5 variables tienen el mismo valor en cada paso de tiempo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NuP2TCpdkj2B",
    "outputId": "c46dafae-66ce-4dc9-be31-cb7ed1b943e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 1., 1.],\n",
      "        [2., 2., 2., 2., 2.],\n",
      "        [3., 3., 3., 3., 3.]])\n",
      "torch.Size([3, 5])\n"
     ]
    }
   ],
   "source": [
    "## secuencia de una instancia, con 3 pasos de tiempo, con 5 features de entrada\n",
    "x_seq = torch.tensor([[1.0]*5, [2.0]*5, [3.0]*5]).float()\n",
    "print(x_seq)\n",
    "print(x_seq.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D8sentCsmMMV"
   },
   "source": [
    "Si vamos a utilizar este tensor como entrada a una red neuronal, es necesario crear el eje del batch que espera toda capa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ab8YtRSWl7km",
    "outputId": "6c1da29a-ba56-483d-e6f0-bf7eb7f215be"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 1., 1., 1.],\n",
       "         [2., 2., 2., 2., 2.],\n",
       "         [3., 3., 3., 3., 3.]]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.reshape(x_seq, (1, 3, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lfLZY4FLwW7G"
   },
   "source": [
    "Vamos a procesar ese batch (de **1 única instancia**) a través del módulo recurrente creado, que espera 5 variables, y que no tiene ninguna restricción con respecto al número de pasos de tiempo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c1-YbES_kw5E",
    "outputId": "a3623870-6979-43d4-c47c-ac549a78be4e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.3520,  0.5253],\n",
      "         [-0.6842,  0.7607],\n",
      "         [-0.8649,  0.9047]]], grad_fn=<TransposeBackward1>)\n",
      "torch.Size([1, 3, 2])\n",
      "tensor([[[-0.8649,  0.9047]]], grad_fn=<StackBackward0>)\n",
      "torch.Size([1, 1, 2])\n"
     ]
    }
   ],
   "source": [
    "## output of the simple RNN:\n",
    "output, hn = rnn_layer(torch.reshape(x_seq, (1, 3, 5)))\n",
    "print(output)\n",
    "print(output.shape)\n",
    "print(hn)\n",
    "print(hn.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GiEQ2eoll4fE"
   },
   "source": [
    "El procesamiento del batch (de 1 única instancia) produce dos tensores:\n",
    "- un **tensor de salidas**: La única capa recurrente tiene dos neuronas, la secuencia tiene un largo de 3 pasos. El tensor de salida incluirá la secuencia de los outputs progresivos de los 3 pasos. Es por eso que tiene una organización [batch=1, pasos=3, neuronas=2]. En una capa recurrente siempre se conservarán los pasos de tiempo de entrada, a menos que se se especifique `return_sequences=False` para que ignore todos los pasos de tiempo menos el último.\n",
    "- un **tensor de estados escondidos**, con el último valor de salida de la celda, pero con el mismo rango. En a capa recurrente, en cada paso de tiempo solo se considera directamente el valor previo, no la secuencia completa.\n",
    "\n",
    "**IMPORTANTE**: El proceso interno de la secuencia solo retorna la salida al haber acabado de procesar la totalidad de los pasos de tiempo de la secuencia. No hay una producción paso por paso de resultados parciales en modo \"streaming\". Esto tiene implicaciones importantes en el tiempo del proceso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PmMVoHQmkz50"
   },
   "source": [
    "Podemos hacer el proceso a pie.\n",
    "Veamoslo para el primer paso de tiempo. Recordemos el valor del tensor con los outputs de los 3 pasos que produce la capa recurrente para compararlos con los resultados obtenidos \"a mano\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pHuTcADawW7G",
    "outputId": "395ad294-1590-4c1a-85ac-bbc667955c13"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.reshape(x_seq[0], (1, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xk4vwMxbwW7G",
    "outputId": "28707508-9759-4cfc-81e4-a001ed4809b5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([-0.0516, -0.0637], requires_grad=True)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_xh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-gjsbPyJM7MV",
    "outputId": "e0607f01-9903-4e8a-d034-01efd9bc087f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.05155426263809204, -0.0636589527130127]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i.item() for i in b_xh]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HsvNDOScwW7G",
    "outputId": "ab4ca5de-280e-4f45-d977-164795953cac"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.10249084234237671, -0.0028247833251953125]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i.item() for i in b_hh]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X8-9LV7bMtTM",
    "outputId": "da2a9b9c-62d5-47e6-af7d-7b139e8aaf44"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.19612378, 0.03488284], dtype=float32),\n",
       " array([ 0.2582553 , -0.27556023], dtype=float32)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i.detach().numpy() for i in w_hh]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inicialmente lo haremos de tal manera que los cálculos correspondan a la convención que hemos seguido: la matriz de pesos en una capa tiene tantas filas como neuronas y tantas columnas como features de entrada (neuronas de la capa anterior).\n",
    "Esto va a implicar modificar las formas de los inputs y de los sesgos para que las operaciones de algebra lineal se puedan realizar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1.],\n",
       "        [2., 2., 2., 2., 2.],\n",
       "        [3., 3., 3., 3., 3.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La capa de entrada tiene 5 features (5 \"neuronas\" en la capa de entrada), por lo que cada paso de tiempo deberia ser un tensor de (5, 1) para poder multiplicar la matriz de pesos (2, 5) por el tensor de entrada. Transpondremos los tensores de entrada con `torch.reshape()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_seq[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.reshape(x_seq[0], (5, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los tensores de los sesgos son de rango 1. Deberíamos tener un sesgo por cada neurona de la capa, organizados entonces en tensores (2, 1). Agregamos el nuevo eje interno con `torch.unsqueeze()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([-0.0516, -0.0637], requires_grad=True)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_xh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0516],\n",
       "        [-0.0637]], grad_fn=<UnsqueezeBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.unsqueeze(b_xh, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input transpuesto:\n",
      " tensor([[1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.]])\n",
      "matriz Wxh:\n",
      " Parameter containing:\n",
      "tensor([[ 0.3643, -0.3121, -0.1371,  0.3319, -0.6657],\n",
      "        [ 0.4241, -0.1455,  0.3597,  0.0983, -0.0866]], requires_grad=True)\n",
      "sesgos bxh:\n",
      " Parameter containing:\n",
      "tensor([-0.0516, -0.0637], requires_grad=True)\n",
      "sesgos bxh modificados:\n",
      " tensor([[-0.0516],\n",
      "        [-0.0637]], grad_fn=<UnsqueezeBackward0>)\n",
      "\n",
      "comb lineal x*Wxh + bhh:\n",
      " tensor([[-0.4702],\n",
      "        [ 0.5864]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "paso=0\n",
    "\n",
    "# input del primer paso\n",
    "xt = torch.reshape(x_seq[0], (5, 1))\n",
    "print(\"input transpuesto:\\n\", xt)\n",
    "# input del primer paso\n",
    "print(\"matriz Wxh:\\n\", w_xh)\n",
    "# input del primer paso\n",
    "print(\"sesgos bxh:\\n\",b_xh)\n",
    "print(\"sesgos bxh modificados:\\n\",torch.unsqueeze(b_xh, 1))\n",
    "# ht combinación lineal: x*Wxh + bxh\n",
    "ht = torch.matmul(w_xh, xt)+torch.unsqueeze(b_xh, 1)\n",
    "print(\"\\ncomb lineal x*Wxh + bhh:\\n\", ht)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para simplificar la preparación de los tensores vamos a hacer la operación análoga de en vez de multiplicar la matriz por las entradas, vamos a cambiar el orden y multiplicar las entradas por la matriz, de esta manera se simplifica el encadenamiento de las operaciones paso a paso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6LYIRQ7lskHJ",
    "outputId": "557a112a-3e27-491a-b36c-3f32ef94f32c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:\n",
      " tensor([[1., 1., 1., 1., 1.]])\n",
      "matriz Wxh:\n",
      " Parameter containing:\n",
      "tensor([[ 0.3643, -0.3121, -0.1371,  0.3319, -0.6657],\n",
      "        [ 0.4241, -0.1455,  0.3597,  0.0983, -0.0866]], requires_grad=True)\n",
      "matriz Wxh transpuesta:\n",
      " tensor([[ 0.3643,  0.4241],\n",
      "        [-0.3121, -0.1455],\n",
      "        [-0.1371,  0.3597],\n",
      "        [ 0.3319,  0.0983],\n",
      "        [-0.6657, -0.0866]], grad_fn=<TransposeBackward0>)\n",
      "sesgos bxh:\n",
      " Parameter containing:\n",
      "tensor([-0.0516, -0.0637], requires_grad=True)\n",
      "\n",
      "comb lineal x*Wxh + bhh:\n",
      " tensor([[-0.4702,  0.5864]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "paso=0\n",
    "\n",
    "# input del primer paso\n",
    "xt = torch.reshape(x_seq[0], (1, 5))\n",
    "print(\"input:\\n\", xt)\n",
    "print(\"matriz Wxh:\\n\", w_xh)\n",
    "print(\"matriz Wxh transpuesta:\\n\", torch.transpose(w_xh, 0, 1))\n",
    "print(\"sesgos bxh:\\n\",b_xh)\n",
    "# ht combinación lineal: x*Wxh + bxh\n",
    "ht = torch.matmul(xt, torch.transpose(w_xh, 0, 1)) + b_xh\n",
    "print(\"\\ncomb lineal x*Wxh + bhh:\\n\", ht)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "60E-4GaxwW7G",
    "outputId": "d888be34-8dad-4d1a-cd61-d9e59cb053f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "input ht-1:\n",
      " tensor([[0., 0.]])\n",
      "matriz Whh transpuesta:\n",
      " tensor([[ 0.1961,  0.2583],\n",
      "        [ 0.0349, -0.2756]], grad_fn=<TransposeBackward0>)\n",
      "sesgos bhh:\n",
      " Parameter containing:\n",
      "tensor([ 0.1025, -0.0028], requires_grad=True)\n",
      "\n",
      "salida combinación lineal:\n",
      " tensor([[-0.3677,  0.5836]], grad_fn=<AddBackward0>)\n",
      "salida activación:\n",
      " tensor([[-0.3520,  0.5253]], grad_fn=<TanhBackward0>)\n"
     ]
    }
   ],
   "source": [
    "prev_h = torch.zeros((ht.shape))\n",
    "print(\"\\ninput ht-1:\\n\", prev_h)\n",
    "print(\"matriz Whh transpuesta:\\n\", torch.transpose(w_hh, 0, 1))\n",
    "print(\"sesgos bhh:\\n\",b_hh)\n",
    "# salida\n",
    "ot = ht + torch.matmul(prev_h, torch.transpose(w_hh, 0, 1)) + b_hh\n",
    "print(\"\\nsalida combinación lineal:\\n\", ot)\n",
    "ot = torch.tanh(ot)\n",
    "print(\"salida activación:\\n\", ot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos comprobar a mano que para el primer paso de procesamiento de la secuencia de entrada obtenemos los mismos valores que encuentra la capa al procesarla internamente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2-2CuJedzLLz"
   },
   "source": [
    "Vamos a hacer los cálculos para los 3 pasos de la secuencia en un ciclo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HaegM7kmkqOw",
    "outputId": "5cedb670-21fa-40a6-f050-f47227d6aebc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time step 0 =>\n",
      " Input : [[1. 1. 1. 1. 1.]]\n",
      "   Hidden : [[-0.4701929  0.5863904]]\n",
      "   prev : [[0. 0.]]\n",
      "   hh : [[ 0.10249084 -0.00282478]]\n",
      "\n",
      " Output (manual) : [[-0.3519801   0.52525216]]\n",
      " RNN output : [[-0.3519801   0.52525216]]\n",
      "\n",
      "Time step 1 =>\n",
      " Input : [[2. 2. 2. 2. 2.]]\n",
      "   Hidden : [[-0.88883156  1.2364397 ]]\n",
      "   prev : [[-0.3519801   0.52525216]]\n",
      "   hh : [[ 0.05178147 -0.23846412]]\n",
      "\n",
      " Output (manual) : [[-0.68424344  0.76074266]]\n",
      " RNN output : [[-0.68424344  0.76074266]]\n",
      "\n",
      "Time step 2 =>\n",
      " Input : [[3. 3. 3. 3. 3.]]\n",
      "   Hidden : [[-1.3074701  1.886489 ]]\n",
      "   prev : [[-0.68424344  0.76074266]]\n",
      "   hh : [[-0.00516871 -0.38916472]]\n",
      "\n",
      " Output (manual) : [[-0.8649416   0.90466356]]\n",
      " RNN output : [[-0.8649416   0.90466356]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## manually computing the output:\n",
    "out_man = []\n",
    "for t in range(3):\n",
    "    xt = torch.reshape(x_seq[t], (1, 5))\n",
    "    print(f'Time step {t} =>')\n",
    "    print(' Input :', xt.numpy())\n",
    "\n",
    "    ht = torch.matmul(xt, torch.transpose(w_xh, 0, 1)) + b_xh\n",
    "    print('   Hidden :', ht.detach().numpy())\n",
    "\n",
    "    if t > 0:\n",
    "        prev_h = out_man[t-1]\n",
    "    else:\n",
    "        prev_h = torch.zeros((ht.shape))\n",
    "\n",
    "    print('   prev :', prev_h.detach().numpy())\n",
    "    # print('   w_hh :', torch.transpose(w_hh, 0, 1))\n",
    "    # print('   out_man :', out_man)\n",
    "    # print('   b_hh :', b_hh.detach().numpy())\n",
    "    temp = torch.matmul(prev_h, torch.transpose(w_hh, 0, 1))  + b_hh\n",
    "    print('   hh :', temp.detach().numpy())\n",
    "\n",
    "\n",
    "    ot = ht + torch.matmul(prev_h, torch.transpose(w_hh, 0, 1)) + b_hh\n",
    "    ot = torch.tanh(ot)\n",
    "    out_man.append(ot)\n",
    "    print('\\n Output (manual) :', ot.detach().numpy())\n",
    "    print(' RNN output :', output[:, t].detach().numpy())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MuJ_8o9Ikd4P"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
